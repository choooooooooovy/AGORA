# AGORA 프로젝트 - 웹 기획자를 위한 가이드

## 📌 프로젝트 한 줄 요약
**"AI 에이전트들이 토론하며 사용자에게 최적의 전공(또는 선택지)을 추천하는 의사결정 시스템"**

---

## 🎯 서비스 목적

### 해결하려는 문제
- 전공/진로 선택 시 **수많은 고려 요소**를 체계적으로 비교하기 어려움
- 개인의 **흥미, 적성, 가치관**을 종합적으로 반영한 의사결정 필요
- 단순 점수 비교가 아닌 **논리적 근거**가 있는 추천 요구

### 제공하는 가치
1. **객관적 분석**: 3개의 AI 에이전트가 서로 다른 관점에서 토론
2. **투명한 과정**: 모든 토론 내용과 점수 산출 근거를 기록
3. **과학적 방법론**: AHP(계층분석법) + TOPSIS(다기준 의사결정) 활용

---

## 👤 사용자 여정 (User Flow)

```
[1단계] 사용자 입력
   ↓
[2단계] AI 페르소나 자동 생성
   ↓
[3단계] Round 1 - 평가 기준 선정 (13턴 토론)
   ↓
[4단계] Round 2 - 기준별 가중치 계산 (13턴 토론 × N회)
   ↓
[5단계] Round 3 - 전공별 점수 산정 (13턴 토론 × M회)
   ↓
[6단계] Round 4 - 최종 순위 도출
   ↓
[결과] 추천 전공 + 상세 근거 제공
```

---

## 📝 입력 데이터 (사용자가 제공하는 정보)

### 필수 입력 3가지
| 항목 | 설명 | 예시 |
|------|------|------|
| **interests** (흥미) | 좋아하는 활동, 관심사 | "복잡한 수학 문제를 푸는 과정이 즐겁고, 프로그래밍으로 알고리즘을 구현하는 것에 흥미가 있습니다." |
| **aptitudes** (적성) | 잘하는 것, 강점 | "논리적 사고력이 뛰어나고 코딩 능력이 우수합니다. 수학 경시대회에서 입상한 경험이 있습니다." |
| **core_values** (가치관) | 중요하게 생각하는 것 | "높은 연봉과 빠른 커리어 성장을 원하며, 워라밸도 중요하게 생각합니다." |

### 선택지 입력
- **candidate_majors**: 비교할 전공 목록 (예: `["컴퓨터공학", "전기전자공학", "산업공학", "경영학", "데이터사이언스"]`)

### 설정값 (옵션)
- `max_criteria`: 평가 기준 최대 개수 (기본값: 5)
- `cr_threshold`: AHP 일관성 임계값 (기본값: 0.1)
- `enable_streaming`: 실시간 스트리밍 여부 (기본값: false)

---

## 🤖 핵심 메커니즘

### 1. **동적 페르소나 생성** (AI가 자동으로 3개 관점 추출)

사용자의 입력 텍스트를 분석하여 **서로 대척되는 3가지 관점**을 가진 AI 에이전트를 생성합니다.

**예시**:
- **Agent 1 (CareerMaximizer)**: "경제적 성공과 빠른 성장" 중시
- **Agent 2 (BalanceSeeker)**: "워라밸과 지속 가능한 커리어" 중시
- **Agent 3 (ImpactDriven)**: "사회적 영향력과 의미 있는 일" 중시

> 💡 **기획 포인트**: 사용자마다 다른 페르소나가 생성됩니다! (개인화)

---

### 2. **4라운드 토론 시스템**

각 라운드마다 **13턴의 구조화된 토론**이 진행됩니다.

#### **Round 1: 평가 기준 선정** (1회 실행)
- **목적**: 전공을 평가할 기준 5개 선정
- **출력 예시**: 
  1. 경제적 성장 잠재력
  2. 사회적 영향력 가능성
  3. 워라밸과 지속 가능한 전문성 발전
  4. 장기적 커리어 안정성
  5. 프로젝트의 사회적 가치 평가

#### **Round 2: 기준별 가중치 계산** (AHP 방식)
- **목적**: 5개 기준의 상대적 중요도 계산
- **방법**: 기준 쌍(A vs B)을 비교하는 토론 10회 실행
- **출력 예시**:
  - 경제적 성장 잠재력: **37.5%**
  - 사회적 영향력 가능성: **20.6%**
  - 워라밸과 지속 가능한 전문성: **18.1%**
  - 장기적 커리어 안정성: **12.6%**
  - 프로젝트의 사회적 가치: **11.3%**

#### **Round 3: 전공별 점수 산정** (Decision Matrix)
- **목적**: 각 전공을 5개 기준으로 평가 (1~10점)
- **방법**: (전공 × 기준) 조합마다 토론 (5개 전공 × 5개 기준 = 25회)
- **출력 예시**:

| 전공 | 경제성 | 사회적 영향 | 워라밸 | 안정성 | 사회적 가치 |
|------|--------|------------|--------|--------|------------|
| 컴퓨터공학 | 8.5 | 7.0 | 6.0 | 6.5 | 7.5 |
| 데이터사이언스 | 7.0 | 8.0 | 6.5 | 6.0 | 7.0 |
| 산업공학 | 6.0 | 6.5 | 7.0 | 7.5 | 6.5 |
| 전기전자공학 | 6.5 | 5.5 | 5.0 | 5.5 | 6.0 |
| 경영학 | 5.5 | 5.0 | 4.5 | 5.0 | 5.5 |

#### **Round 4: 최종 순위 도출** (TOPSIS)
- **목적**: 가중치 + 점수를 종합하여 최종 순위 계산
- **출력 예시**:
  1. 🥇 **컴퓨터공학** (근접도: 0.7954)
  2. 🥈 **데이터사이언스** (근접도: 0.6151)
  3. 🥉 **산업공학** (근접도: 0.4251)
  4. **전기전자공학** (근접도: 0.2800)
  5. **경영학** (근접도: 0.0000)

---

### 3. **13턴 토론 구조** (모든 라운드 공통)

각 토론은 다음 순서로 진행됩니다:

```
턴 1-3:   Agent들이 초기 제안 제시
턴 4-6:   서로의 제안을 비판
턴 7-9:   반론 및 방어
턴 10-12: 최종 조율 및 합의 시도
턴 13:    Director가 최종 결정
```

> 💡 **기획 포인트**: 각 턴마다 구체적 근거(숫자, 통계, 사용자 키워드 인용)를 강제화하여 품질 보장

---

## 📊 출력 데이터 (시스템이 제공하는 결과)

### 1. **최종 순위** (JSON)
```json
{
  "final_ranking": [
    {
      "rank": 1,
      "major": "컴퓨터공학",
      "closeness_coefficient": 0.7954,
      "weighted_scores": {
        "경제적 성장 잠재력": 3.19,
        "사회적 영향력 가능성": 1.44,
        "워라밸과 지속 가능한 전문성 발전": 1.09,
        "장기적 커리어 안정성": 0.82,
        "프로젝트의 사회적 가치 평가": 0.85
      }
    },
    ...
  ]
}
```

### 2. **토론 기록** (각 라운드마다)
- 13턴의 모든 발언 내용
- 각 에이전트의 주장과 근거
- Director의 최종 판단 이유

### 3. **품질 메트릭**
- AHP 일관성 비율 (CR): 0.0214 (기준: <0.1)
- 사용자 키워드 인용률: 100%
- 평균 발언 길이: 650자+

---

## 🛠️ 기술 스택 (웹 개발 관련)

| 구분 | 기술 | 용도 |
|------|------|------|
| **Backend** | Python 3.9 | 핵심 로직 |
| **AI Framework** | LangChain + LangGraph | 멀티 에이전트 오케스트레이션 |
| **LLM** | OpenAI GPT-4o | 토론 생성 |
| **수학 라이브러리** | NumPy, SciPy | AHP/TOPSIS 계산 |
| **데이터 포맷** | JSON | 입출력 형식 |

### 웹 연동 시 고려사항

#### API 설계 예시
```
POST /api/analyze
{
  "interests": "...",
  "aptitudes": "...",
  "core_values": "...",
  "candidate_majors": ["..."]
}

→ Response:
{
  "session_id": "32c97f10-...",
  "status": "processing",
  "estimated_time": "120 seconds"
}

GET /api/results/{session_id}
→ Response: {최종 순위, 토론 기록, 메트릭}
```

#### 처리 시간
- **전체 프로세스**: 약 2~3분 (LLM API 호출 다수 포함)
- **Round 1**: ~20초 (1회 토론)
- **Round 2**: ~40초 (10회 토론)
- **Round 3**: ~60초 (25회 토론)
- **Round 4**: ~5초 (수학 계산만)

> ⚠️ **주의**: 비동기 처리 필수! (WebSocket, Polling, 또는 Queue 사용 권장)

---

## 🎨 UI/UX 권장사항

### 1. **입력 화면**
- 텍스트 영역 3개 (각 최소 10자 이상 입력 가이드)
- 전공 선택 (멀티 셀렉트, 최소 2개 이상)
- "분석 시작" 버튼

### 2. **진행 상황 표시**
```
[진행 중] Round 1/4 - 평가 기준 선정 중... (20초 소요)
[완료] Round 1 완료 - 5개 기준 선정됨
[진행 중] Round 2/4 - 기준별 가중치 계산 중... (진행률: 3/10)
```

### 3. **결과 화면**
- **요약 카드**: 1~5위 전공 표시
- **상세 보기**: 
  - 각 전공의 강점/약점
  - 5개 기준별 점수 비교 (레이더 차트 추천)
  - 주요 토론 내용 하이라이트
- **토론 기록**: 펼침/접힘 형식으로 13턴 내용 제공

### 4. **시각화 추천**
- 📊 **가로 막대 그래프**: 최종 순위 (근접도 계수)
- 🕸️ **레이더 차트**: 전공별 5개 기준 점수 비교
- 🥧 **파이 차트**: 기준별 가중치 분포
- 📈 **히트맵**: Decision Matrix (전공 × 기준)

---

## 💡 차별화 포인트

### 경쟁 서비스 대비 강점
1. **투명성**: "왜 1위인가?" 질문에 대한 명확한 답변 제공
2. **개인화**: 사용자마다 다른 AI 관점 생성 (One-size-fits-all 아님)
3. **과학적**: 감에 의존하지 않는 수학적 방법론
4. **품질 보장**: 모든 주장에 구체적 근거 요구 (키워드 인용, 통계 수치)

### 확장 가능성
- ✅ 전공 선택 → 직장 선택, 투자 대안, 제품 비교 등으로 확장
- ✅ 토론 라운드 커스터마이징 (3라운드 간소화 vs 5라운드 심층 분석)
- ✅ 다국어 지원 (프롬프트만 번역하면 됨)

---

## 📂 프로젝트 구조 (간단 버전)

```
Prioritization/
│
├── data/
│   └── user_inputs/
│       └── current_user.json          # 사용자 입력 샘플
│
├── core/
│   ├── persona_generator.py           # AI 페르소나 자동 생성
│   └── workflow_engine.py             # 전체 흐름 제어
│
├── workflows/
│   ├── round1_criteria.py             # Round 1: 기준 선정
│   ├── round2_ahp.py                  # Round 2: 가중치 계산
│   ├── round3_scoring.py              # Round 3: 점수 산정
│   └── round4_topsis.py               # Round 4: 최종 순위
│
├── utils/
│   ├── ahp_calculator.py              # AHP 수학 계산
│   └── topsis_calculator.py           # TOPSIS 수학 계산
│
├── output/
│   ├── round1_*.json                  # 기준 선정 결과
│   ├── round2_*.json                  # 가중치 결과
│   ├── round3_*.json                  # 점수 결과
│   └── round4_*.json                  # 최종 순위
│
└── templates/
    ├── system_prompts.yaml            # AI 에이전트 기본 설정
    └── round_prompts.yaml             # 라운드별 프롬프트
```

---

## 🚀 빠른 시작 (개발자용)

### 1. 환경 설정
```bash
pip install -r requirements.txt
export OPENAI_API_KEY="your-api-key"
```

### 2. 실행
```bash
python round1_debate.py  # Round 1
python round2_debate.py  # Round 2
python round3_debate.py  # Round 3
python round4_debate.py  # Round 4
```

### 3. 결과 확인
```bash
cat output/round4_*.json | jq '.final_ranking'
```

---

## ❓ FAQ (웹 기획 관점)

### Q1. 실시간으로 토론 과정을 보여줄 수 있나요?
A. 가능합니다! `enable_streaming: true` 설정 시 턴마다 결과를 WebSocket으로 전송할 수 있습니다.

### Q2. 모바일에서도 사용할 수 있나요?
A. 입력 화면은 모바일 친화적이지만, 결과 화면(특히 차트)은 태블릿 이상 권장합니다.

### Q3. 한 번에 몇 명의 사용자를 처리할 수 있나요?
A. OpenAI API Rate Limit에 의존합니다. 현재는 순차 처리이므로 동시 사용자를 위해 Queue 시스템 필요합니다.

### Q4. 결과를 PDF로 다운로드할 수 있나요?
A. JSON 데이터를 기반으로 PDF 생성 기능 추가 가능합니다 (프론트엔드에서 html2pdf 등 활용).

### Q5. 사용자가 토론 과정에 개입할 수 있나요?
A. 현재는 자동 실행이지만, "사용자 피드백 턴" 추가 가능합니다 (예: Round 2 후 가중치 수정 허용).

---

## 📞 연락처

프로젝트 관련 문의: AGORA Team  
Repository: choooooooooovy/AGORA

---

**마지막 업데이트**: 2025년 11월 10일
